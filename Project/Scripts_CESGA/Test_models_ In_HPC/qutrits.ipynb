{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fd371-4b41-48dc-ac4f-bc6a1c2559e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import numpy as np\n",
    "import json\n",
    "from decimal import Decimal\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.linalg import expm\n",
    "import json\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "from scipy.linalg import qr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1894fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cargar_datos_json(json_path, num_jets=10000, num_constituents=10):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    eventos = []\n",
    "    for i, evento in enumerate(data[:num_jets]):\n",
    "        # Extract jet kinematics\n",
    "        jet_pt = evento.get('jet_pt', i)\n",
    "        jet_eta = evento.get('jet_eta', i)\n",
    "        jet_phi = evento.get('jet_phi', i)\n",
    "        jet_mass = evento.get('jet_sdmass', i)\n",
    "        jet_energy = evento.get('jet_energy', i)\n",
    "        jet_tau1 = evento.get('jet_tau1', i)\n",
    "        jet_tau2 = evento.get('jet_tau2', i)\n",
    "        jet_tau3 = evento.get('jet_tau3', i)    \n",
    "        jet_tau4 = evento.get('jet_tau4', i)\n",
    "\n",
    "        jet_tau12 = jet_tau1 / jet_tau2 if jet_tau2 != 0 else 0\n",
    "        jet_tau23 = jet_tau2 / jet_tau3 if jet_tau3 != 0 else 0\n",
    "        jet_tau34 = jet_tau3 / jet_tau4 if jet_tau4 != 0 else 0\n",
    "\n",
    "        # Extract constituents (particles)\n",
    "        part_px = np.array(evento.get('part_px', []))\n",
    "        part_py = np.array(evento.get('part_py', []))\n",
    "        part_pz = np.array(evento.get('part_pz', []))\n",
    "        part_energy = np.array(evento.get('part_energy', []))\n",
    "        part_d0val = np.array(evento.get('part_d0val', []))\n",
    "        part_dzval = np.array(evento.get('part_dzval', []))\n",
    "        \n",
    "        # Calculate pT, eta, phi, mass for each constituent\n",
    "        pt = np.sqrt(part_px**2 + part_py**2)\n",
    "        p_total = np.sqrt(part_px**2 + part_py**2 + part_pz**2)\n",
    "        eta = 0.5 * np.log((p_total + part_pz) / (p_total - part_pz + 1e-8))  # Avoiding dividing by 0\n",
    "        phi = np.arctan2(part_py, part_px)\n",
    "        mass = np.sqrt(np.maximum(0, part_energy**2 - (part_px**2 + part_py**2 + part_pz**2)))\n",
    "        \n",
    "        # Seleccionar los num_constituents con mayor pt\n",
    "        indices_ordenados = np.argsort(pt)[::-1][:num_constituents]\n",
    "        \n",
    "        top_constituents = []\n",
    "        for idx in indices_ordenados:\n",
    "            top_constituents.append({\n",
    "                'pt': pt[idx],\n",
    "                'eta': eta[idx],\n",
    "                'phi': phi[idx],\n",
    "                'px': part_px[idx],\n",
    "                'py': part_py[idx],\n",
    "                'pz': part_pz[idx],\n",
    "                'mass': mass[idx],\n",
    "                'energy': part_energy[idx],\n",
    "                'd0': part_d0val[idx],\n",
    "                'dz': part_dzval[idx]\n",
    "            })\n",
    "            \n",
    "        eventos.append({\n",
    "            'pt_jet': jet_pt,\n",
    "            'eta_jet': jet_eta,\n",
    "            'phi_jet': jet_phi,\n",
    "            'mass_jet': jet_mass,\n",
    "            'energy_jet': jet_energy,\n",
    "            'tau1_jet': jet_tau1,\n",
    "            'tau2_jet': jet_tau2,\n",
    "            'tau3_jet': jet_tau3,\n",
    "            'tau4_jet': jet_tau4,\n",
    "            'tau12_jet': jet_tau12,\n",
    "            'tau23_jet': jet_tau23,\n",
    "            'tau34_jet': jet_tau34,\n",
    "            'constituents': top_constituents\n",
    "        })\n",
    "\n",
    "    return eventos\n",
    "# Ejemplo de uso\n",
    "datos_HToBB = cargar_datos_json('./HToBB_120_flat.json', num_jets=10000, num_constituents=10)\n",
    "datos_TTBar = cargar_datos_json('./TTBar_120_flat.json', num_jets=10000, num_constituents=10)\n",
    "datos_WToqq = cargar_datos_json('./WToQQ_120_flat.json', num_jets=10000, num_constituents=10)\n",
    "datos_QCD_simu_1 = cargar_datos_json('./ZJetsToNuNu_120_flat.json', num_jets=22500, num_constituents=10)\n",
    "datos_QCD_simu_2 = cargar_datos_json('./ZJetsToNuNu_120_flat.json', num_jets=22500, num_constituents=10)\n",
    "datos_QCD_simu_full = datos_QCD_simu_1 + datos_QCD_simu_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = np.array(datos_QCD_simu_full)\n",
    "\n",
    "# Separar 10,000 para entrenamiento y 12,500 restantes\n",
    "X_train, X_temp = train_test_split(\n",
    "    datos, \n",
    "    train_size=10000, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Separar 2,500 para validación y 10,000 para inferencia\n",
    "X_val, rest = train_test_split(\n",
    "    X_temp, \n",
    "    train_size=2500, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_inf, rest = train_test_split(\n",
    "    rest, \n",
    "    train_size=10000, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Verificar tamaños\n",
    "print(f\"Entrenamiento: {len(X_train)}\")\n",
    "print(f\"Validación: {len(X_val)}\")\n",
    "print(f\"Inferencia: {len(X_inf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda53dd-32c1-4dab-ace1-78839e881149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Gell-Mann matrices (SU(3))\n",
    "Lambda = {\n",
    "    1: torch.tensor([[0, 1, 0],\n",
    "                 [1, 0, 0],\n",
    "                 [0, 0, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    2: torch.tensor([[0, -1j, 0],\n",
    "                 [1j, 0, 0],\n",
    "                 [0, 0, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    3: torch.tensor([[1, 0, 0],\n",
    "                 [0, -1, 0],\n",
    "                 [0, 0, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    4: torch.tensor([[0, 0, 1],\n",
    "                 [0, 0, 0],\n",
    "                 [1, 0, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    5: torch.tensor([[0, 0, -1j],\n",
    "                 [0, 0, 0],\n",
    "                 [1j, 0, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    6: torch.tensor([[0, 0, 0],\n",
    "                 [0, 0, 1],\n",
    "                 [0, 1, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    7: torch.tensor([[0, 0, 0],\n",
    "                 [0, 0, -1j],\n",
    "                 [0, 1j, 0]], dtype=torch.cdouble),\n",
    "\n",
    "    8: (1/torch.sqrt(torch.tensor(3.0))) * torch.tensor([[1, 0, 0],\n",
    "                                  [0, 1, 0],\n",
    "                                  [0, 0, -2]], dtype=torch.cdouble),\n",
    "    0: torch.tensor([[1, 0, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [0, 0, 1]], dtype=torch.cdouble)\n",
    "}\n",
    "\n",
    "# Spin-1 rotation generators (SO(3) ⊂ SU(3))\n",
    "Sigma = {\n",
    "    1: (1 / torch.sqrt(torch.tensor(2.0))) * torch.tensor([\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 0]\n",
    "    ], dtype=torch.cdouble),\n",
    "\n",
    "    2: (1 / torch.sqrt(torch.tensor(2.0))) * torch.tensor([\n",
    "        [0, -1j, 0],\n",
    "        [1j, 0, -1j],\n",
    "        [0, 1j, 0]\n",
    "    ], dtype=torch.cdouble),\n",
    "\n",
    "    3: torch.tensor([\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, -1]\n",
    "    ], dtype=torch.cdouble)\n",
    "}\n",
    "\n",
    "def TSWAP_matrix():\n",
    "    tswap = np.zeros((9, 9), dtype=complex)\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ket = np.zeros(9)\n",
    "            bra = np.zeros(9)\n",
    "            ket[3*i + j] = 1   # |i⟩|j⟩\n",
    "            bra[3*j + i] = 1   # |j⟩|i⟩\n",
    "            tswap += np.outer(bra, ket)\n",
    "    return tswap\n",
    "\n",
    "\n",
    "def unitary_from_generator(generator_matrix, theta):\n",
    "    if not torch.is_tensor(theta):\n",
    "        theta = torch.tensor(theta, dtype=torch.cdouble)\n",
    "    i = torch.tensor(1j, dtype=torch.cdouble)\n",
    "    return Lambda[0] + (torch.cos(theta) - torch.tensor(1.0)) * generator_matrix @ generator_matrix + i * torch.sin(theta) * generator_matrix\n",
    "\n",
    "def inicializing_qutrit_state(theta1, theta2, phi1, phi2):\n",
    "    Gamma= 0\n",
    "    a0 = 0\n",
    "    a1 = 0\n",
    "    a2 = 0\n",
    "\n",
    "    Gamma = torch.sqrt(torch.tensor(2.0)) * (torch.tensor(3.0) + torch.cos(theta1)*torch.cos(theta2) + torch.sin(theta1)*torch.sin(theta2)*torch.cos(phi1 - phi2))**(torch.tensor(-0.5))\n",
    "\n",
    "    a0 = (torch.sqrt(torch.tensor(2.0)) * torch.cos(theta1/2) * torch.cos(theta2/2)).item()\n",
    "    a1 = (torch.exp(1j * phi1) * torch.sin(theta1/2) * torch.cos(theta2/2) + torch.cos(theta1/2) * torch.sin(theta2/2) * torch.exp(1j * phi2)).item()\n",
    "    a2 = (torch.sqrt(torch.tensor(2.0)) * torch.exp(1j * (phi1 + phi2)) * torch.sin(theta1/2) * torch.sin(theta2/2)).item()\n",
    "\n",
    "    state = Gamma * torch.tensor([a0, a1, a2], dtype=torch.cdouble)\n",
    "    state = state / torch.linalg.norm(state)\n",
    "    \n",
    "    \n",
    "\n",
    "    return state.detach().clone().numpy()\n",
    "\n",
    "\n",
    "\n",
    "def unitary_from_state(psi):\n",
    "    \"\"\"\n",
    "    psi: complex normalized vector of dimension 3 (qutrit)\n",
    "    returns: 3x3 unitary whose first column is psi\n",
    "    \"\"\"\n",
    "    psi = psi / np.linalg.norm(psi)  # por seguridad\n",
    "    \n",
    "    a1 = torch.tensor([0.555,0, 0.555], dtype=torch.cdouble)\n",
    "    a2 = torch.tensor([0.555,0.555, 0], dtype=torch.cdouble)\n",
    "    mat = np.column_stack([psi, a1 , a2])\n",
    "\n",
    "    Q, R = qr(mat)\n",
    "    phase = np.vdot(psi, Q[:,0])\n",
    "    Q[:,0] = Q[:,0] * (phase/abs(phase)).conj()\n",
    "    \n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320bd8f",
   "metadata": {},
   "source": [
    "# **MODEL DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2984f-d388-4216-bd96-4dac11cebca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "from autoray import do\n",
    "\n",
    "\n",
    "num_particles = 4\n",
    "num_latent = 1\n",
    "num_ref = num_particles - num_latent\n",
    "num_trash = num_ref\n",
    "wires = list(range(num_particles + num_ref + 1))  # +1 ancilla\n",
    "ancilla = wires[-1]\n",
    "dev = qml.device(\"default.qutrit\", wires=wires)  \n",
    "\n",
    "latent_wire = 0\n",
    "trash_wires = wires[1:num_particles]\n",
    "ref_wires = wires[num_particles:-1]\n",
    "\n",
    "# Encoding functions\n",
    "def f(w): return 1 + (2 * np.pi / (1 + torch.exp(-w)))\n",
    "def phi_circuit(w, phi, phi_jet, pt, pt_jet): return f(w) * (pt / pt_jet) * (phi - phi_jet)\n",
    "def theta_circuit(w, eta, eta_jet, pt, pt_jet): return f(w) * (pt / pt_jet) * (eta - eta_jet)\n",
    "def mass_circuit(w, mass, mass_jet, pt, pt_jet):  return  f(w) * (pt / pt_jet) * (mass - mass_jet)\n",
    "def energy_circuit(w, energy, energy_jet, pt, pt_jet): return f(w) * (pt / pt_jet) *  (energy - energy_jet)\n",
    "def d0_circuit(w, d0, pt, pt_jet): return f(w) * (pt / pt_jet) * (d0)\n",
    "def dz_circuit(w, dz, pt, pt_jet): return f(w) * (pt / pt_jet) * (dz)\n",
    "def tau12_circuit(w, tau12, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau12)\n",
    "def tau23_circuit(w, tau23, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau23)\n",
    "def tau34_circuit(w, tau34, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau34)\n",
    "def tau1_circuit(w, tau1, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau1)\n",
    "def tau2_circuit(w, tau2, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau2)\n",
    "def tau3_circuit(w, tau3, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau3)\n",
    "def tau4_circuit(w, tau4, pt, pt_jet): return f(w) * (pt / pt_jet) * (tau4)\n",
    "\n",
    "# Encoding for qutrits\n",
    "def encode_1p1q_qutrit(jets, w, unitaries):\n",
    "\n",
    "    pt_jet = jets['pt_jet']\n",
    "    eta_jet = jets['eta_jet']\n",
    "    phi_jet = jets['phi_jet']\n",
    "    mass_jet = jets['mass_jet']\n",
    "    energy_jet = jets['energy_jet']\n",
    "    #tau12_jet = jets['tau12_jet']\n",
    "    #tau23_jet = jets['tau23_jet']\n",
    "    #tau34_jet = jets['tau34_jet']\n",
    "    constituents = jets['constituents']\n",
    "        \n",
    "    for i in range(num_particles):\n",
    "        c = constituents[i]\n",
    "        theta = theta_circuit(w, c['eta'], eta_jet, c['pt'], pt_jet)\n",
    "        phi = phi_circuit(w, c['phi'], phi_jet, c['pt'], pt_jet)\n",
    "        mass = mass_circuit(w, c['mass'], mass_jet, c['pt'], pt_jet)\n",
    "        energy = energy_circuit(w, c['energy'], energy_jet, c['pt'], pt_jet)\n",
    "        d0 = d0_circuit(w, c['d0'], c['pt'], pt_jet)\n",
    "        dz = dz_circuit(w, c['dz'], c['pt'], pt_jet)\n",
    "        #tau12 = tau12_circuit(w,  tau12_jet ,c['pt'], pt_jet)\n",
    "        #tau23 = tau23_circuit(w,  tau23_jet, c['pt'], pt_jet)\n",
    "        #tau34 = tau34_circuit(w,  tau34_jet, c['pt'], pt_jet)\n",
    "        #tau1 = tau1_circuit(w, jets['tau1_jet'], c['pt'], pt_jet)\n",
    "        #tau2 = tau2_circuit(w, jets['tau2_jet'], c['pt'], pt_jet)\n",
    "        #tau3 = tau3_circuit(w, jets['tau3_jet'], c['pt'], pt_jet)\n",
    "        #tau4 = tau4_circuit(w, jets['tau4_jet'], c['pt'], pt_jet)\n",
    "\n",
    "        initial_state = inicializing_qutrit_state( theta, phi, d0, dz)\n",
    "        u = unitary_from_state(initial_state)\n",
    "        unitaries.append(u)\n",
    "        qml.QutritUnitary(u, wires=i)    \n",
    "\n",
    "def decode_from_unitaries(unitaries):\n",
    "    for i, u in enumerate(unitaries):\n",
    "        qml.QutritUnitary(u.conj().T, wires=i)\n",
    "\n",
    "# Variational layer for qutrits (ENCODER)\n",
    "def variational_layer_qutrit(theta_i, phi_i, w_i, num_layers):\n",
    "    for layer in range(num_layers):\n",
    "        for i in range(num_particles):\n",
    "            for j in range(i + 1, num_particles):\n",
    "                qml.TAdd(wires=[i, j])\n",
    "        for i in range(num_particles):\n",
    "\n",
    "            RX = unitary_from_generator(Sigma[1], phi_i[layer, i])\n",
    "            RY = unitary_from_generator(Sigma[2], theta_i[layer, i])\n",
    "            RZ = unitary_from_generator(Sigma[3], w_i[layer, i])\n",
    "    \n",
    "            qml.QutritUnitary(RX, wires=i)\n",
    "            qml.QutritUnitary(RZ, wires=i)\n",
    "            qml.QutritUnitary(RY, wires=i)\n",
    "\n",
    "\n",
    "# DECODER (hermite conjugate encoder)\n",
    "def variational_layer_qutrit_dagger(theta_i, phi_i, w_i, num_layers):\n",
    "    # Recorremos las capas en orden inverso\n",
    "    for layer in reversed(range(num_layers)):\n",
    "\n",
    "        # Invertir las rotaciones locales primero (orden inverso y adjunto)\n",
    "        for i in reversed(range(num_particles)):\n",
    "            RX = unitary_from_generator(Sigma[1], phi_i[layer, i])\n",
    "            RY = unitary_from_generator(Sigma[2], theta_i[layer, i])\n",
    "            RZ = unitary_from_generator(Sigma[3], w_i[layer, i])\n",
    "\n",
    "            # Aplicar el inverso en orden opuesto\n",
    "            qml.QutritUnitary(RY.conj().T, wires=i)\n",
    "            qml.QutritUnitary(RZ.conj().T, wires=i)\n",
    "            qml.QutritUnitary(RX.conj().T, wires=i)\n",
    "\n",
    "        # Ahora invertir los TAdd (mismo gate, pero orden inverso)\n",
    "        for i in reversed(range(num_particles)):\n",
    "            for j in reversed(range(i + 1, num_particles)):\n",
    "                qml.TAdd(wires=[i, j])\n",
    "\n",
    "# QAE circuit for qutrits\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "def qae_circuit_qutrit(jets, w, theta_i, phi_i, w_i, num_layers):\n",
    "    unitaries = []\n",
    "    encode_1p1q_qutrit(jets, w, unitaries)\n",
    "    variational_layer_qutrit(theta_i, phi_i, w_i, num_layers)\n",
    "    tswap = TSWAP_matrix()\n",
    "\n",
    "    for trash_wire, ref_wire in zip(trash_wires, ref_wires):\n",
    "        qml.THadamard(wires=ancilla, subspace=None) #With none they apply the generalized version\n",
    "        qml.ControlledQutritUnitary(tswap, control_wires=ancilla, wires=[trash_wire, ref_wire])\n",
    "        qml.THadamard(wires=ancilla, subspace=None)\n",
    "    \n",
    "    return qml.probs(wires=ancilla)\n",
    "\n",
    "# Cost function\n",
    "def cost_function_with_fidelity_qutrit(jet, w, theta_i, phi_i, w_i, num_layers):\n",
    "    prob_0 = qae_circuit_qutrit(jet, w, theta_i, phi_i, w_i, num_layers)[0]\n",
    "    fidelity = prob_0\n",
    "    return -fidelity, fidelity.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f2e71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bde6651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin etapa  1\n",
      "Fin etapa  2\n",
      "Fin etapa  3\n",
      "Fin etapa  4\n",
      "Fin etapa  5\n",
      "Fin etapa  6\n",
      "Fin etapa  7\n",
      "Fin etapa  8\n",
      "Fin etapa  9\n",
      "Fin etapa  10\n",
      "Fin etapa  11\n",
      "Fin etapa  12\n",
      "Fin etapa  13\n",
      "Fin etapa  14\n",
      "Fin etapa  15\n",
      "Fin etapa  16\n",
      "Fin etapa  17\n",
      "Fin etapa  18\n",
      "Fin etapa  19\n",
      "Fin etapa  20\n",
      "Fin etapa  21\n",
      "Fin etapa  22\n",
      "Fin etapa  23\n",
      "Fin etapa  24\n",
      "Fin etapa  25\n",
      "Fin etapa  26\n",
      "Fin etapa  27\n",
      "Fin etapa  28\n",
      "Fin etapa  29\n",
      "Fin etapa  30\n",
      "Fin etapa  31\n",
      "Fin etapa  32\n",
      "Fin etapa  33\n",
      "Fin etapa  34\n",
      "Fin etapa  35\n",
      "Fin etapa  36\n",
      "Fin etapa  37\n",
      "Fin etapa  38\n",
      "Fin etapa  39\n",
      "Fin etapa  40\n",
      "Fin etapa  41\n",
      "Fin etapa  42\n",
      "Fin etapa  43\n",
      "Fin etapa  44\n",
      "Fin etapa  45\n",
      "Fin etapa  46\n",
      "Fin etapa  47\n",
      "Fin etapa  48\n",
      "Fin etapa  49\n",
      "Fin etapa  50\n",
      "Fin etapa  51\n",
      "Fin etapa  52\n",
      "Fin etapa  53\n",
      "Fin etapa  54\n",
      "Fin etapa  55\n",
      "Fin etapa  56\n",
      "Fin etapa  57\n",
      "Fin etapa  58\n",
      "Fin etapa  59\n",
      "Fin etapa  60\n",
      "Fin etapa  61\n",
      "Fin etapa  62\n",
      "Fin etapa  63\n",
      "Fin etapa  64\n",
      "Fin etapa  65\n",
      "Fin etapa  66\n",
      "Fin etapa  67\n",
      "Fin etapa  68\n",
      "Fin etapa  69\n",
      "Fin etapa  70\n",
      "Fin etapa  71\n",
      "Fin etapa  72\n",
      "Fin etapa  73\n",
      "Fin etapa  74\n",
      "Fin etapa  75\n",
      "Fin etapa  76\n",
      "Fin etapa  77\n",
      "Fin etapa  78\n",
      "Fin etapa  79\n",
      "Fin etapa  80\n",
      "Fin etapa  81\n",
      "Fin etapa  82\n",
      "Fin etapa  83\n",
      "Fin etapa  84\n",
      "Fin etapa  85\n",
      "Fin etapa  86\n",
      "Fin etapa  87\n",
      "Fin etapa  88\n",
      "Fin etapa  89\n",
      "Fin etapa  90\n",
      "Fin etapa  91\n",
      "Fin etapa  92\n",
      "Fin etapa  93\n",
      "Fin etapa  94\n",
      "Fin etapa  95\n",
      "Fin etapa  96\n",
      "Fin etapa  97\n",
      "Fin etapa  98\n",
      "Fin etapa  99\n",
      "Fin etapa  100\n"
     ]
    }
   ],
   "source": [
    "fil_100_back = []\n",
    "fil_100_HToBB = []\n",
    "fil_100_WToQQ = []\n",
    "fil_100_TTBar = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    w = torch.tensor(1.0, requires_grad=True)\n",
    "    num_layers = 1 # Number of variational layers\n",
    "    theta_i = (torch.rand(num_layers, num_particles) * 2 * torch.pi).requires_grad_(True)\n",
    "    phi_i   = (torch.rand(num_layers, num_particles) * 2 * torch.pi).requires_grad_(True)\n",
    "    w_i     = (torch.rand(num_layers, num_particles) * 2 * torch.pi).requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [w, theta_i, phi_i, w_i],\n",
    "        lr=5e-2,              \n",
    "        betas=(0.5, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=0.0,    \n",
    "        amsgrad=True          \n",
    "    )\n",
    "    num_epochs = 1\n",
    "    all_fidelities = []\n",
    "    event_fidelities = []  # List to store event fidelities\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        epoch_fidelities = []\n",
    "        avg_fidelity = 0.0\n",
    "        avg_loss = 0.0\n",
    "        \n",
    "        for jet in X_train:\n",
    "            if len(jet['constituents']) < num_particles:\n",
    "                continue\n",
    "        \n",
    "            loss, fidelity = cost_function_with_fidelity_qutrit(jet, w, theta_i, phi_i, w_i,  num_layers)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            epoch_fidelities.append(fidelity)\n",
    "            event_fidelities.append(fidelity * 100)  # in %\n",
    "\n",
    "        avg_loss = total_loss / len(epoch_fidelities)\n",
    "        avg_fidelity = np.mean(epoch_fidelities) * 100\n",
    "        all_fidelities.append(avg_fidelity)\n",
    "\n",
    "\n",
    "    event_fidelities_back = []\n",
    "    event_fidelities_HToBB = []\n",
    "    event_fidelities_WToQQ = []\n",
    "    event_fidelities_TTBar = []\n",
    "    fidelidades = []\n",
    "    etiquetas = []\n",
    "\n",
    "    \n",
    "    for jet in X_inf:\n",
    "        if len(jet['constituents']) < num_particles:\n",
    "            continue\n",
    "        _, fidelity = cost_function_with_fidelity_qutrit(jet, w, theta_i, phi_i, w_i, num_layers)\n",
    "        event_fidelities_back.append(fidelity * 100) \n",
    "        fidelidades.append(fidelity)\n",
    "        etiquetas.append(0)\n",
    "\n",
    "    \n",
    "\n",
    "    inicio = time.time()\n",
    "    for jet in datos_HToBB:\n",
    "        if len(jet['constituents']) < num_particles:\n",
    "            continue\n",
    "        _, fidelity = cost_function_with_fidelity_qutrit(jet, w, theta_i, phi_i, w_i, num_layers)\n",
    "        event_fidelities_HToBB.append(fidelity * 100) \n",
    "        fidelidades.append(fidelity)\n",
    "        etiquetas.append(1)\n",
    "\n",
    "    \n",
    "\n",
    "    for jet in datos_TTBar:\n",
    "        if len(jet['constituents']) < num_particles:\n",
    "            continue\n",
    "        _, fidelity = cost_function_with_fidelity_qutrit(jet, w, theta_i, phi_i, w_i, num_layers)\n",
    "        event_fidelities_TTBar.append(fidelity * 100) \n",
    "        fidelidades.append(fidelity)\n",
    "        etiquetas.append(1)\n",
    "\n",
    "    \n",
    "\n",
    "    for jet in datos_WToqq:\n",
    "        if len(jet['constituents']) < num_particles:\n",
    "            continue\n",
    "        _, fidelity = cost_function_with_fidelity_qutrit(jet, w, theta_i, phi_i, w_i, num_layers)\n",
    "        event_fidelities_WToQQ.append(fidelity * 100) \n",
    "        fidelidades.append(fidelity)\n",
    "        etiquetas.append(1)\n",
    "\n",
    "    \n",
    "\n",
    "    fil_100_back.append(event_fidelities_back)\n",
    "    fil_100_HToBB.append(event_fidelities_HToBB)\n",
    "    fil_100_WToQQ.append(event_fidelities_WToQQ)\n",
    "    fil_100_TTBar.append(event_fidelities_TTBar)\n",
    "\n",
    "    print(\"Fin etapa \", i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb18e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_100_back_2=fil_100_back,\n",
    "fil_100_HToBB_2=fil_100_HToBB,\n",
    "fil_100_WToQQ_2=fil_100_WToQQ,\n",
    "fil_100_TTBar_2=fil_100_TTBar\n",
    "\n",
    "np.savez(\n",
    "    'fidelidades_100.npz',\n",
    "    fil_100_back_S=fil_100_back_2,\n",
    "    fil_100_HToBB_S=fil_100_HToBB_2,\n",
    "    fil_100_WToQQ_S=fil_100_WToQQ_2,\n",
    "    fil_100_TTBar_S=fil_100_TTBar_2\n",
    ")\n",
    "print(\"Guardado como fidelidades_100.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./fidelidades_100.npz', allow_pickle=True)\n",
    "fil_100_back_3= data['fil_100_back_S']\n",
    "fil_100_HToBB_3 = data['fil_100_HToBB_S']\n",
    "fil_100_WToQQ_3 = data['fil_100_WToQQ_S']\n",
    "fil_100_TTBar_3 = data['fil_100_TTBar_S']\n",
    "\n",
    "fil_100_back_3 = fil_100_back_3.squeeze()\n",
    "fil_100_HToBB_3 = fil_100_HToBB_3.squeeze()\n",
    "fil_100_WToQQ_3 = fil_100_WToQQ_3.squeeze()\n",
    "fil_100_TTBar_3 = fil_100_TTBar_3.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe206f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "means_HToBB = []\n",
    "means_WToQQ = []\n",
    "means_TTBar = []\n",
    "means_back = []\n",
    "\n",
    "median_HToBB = []\n",
    "median_WToQQ = []\n",
    "median_TTBar = []\n",
    "median_back = []\n",
    "\n",
    "for i in range(100):\n",
    "    means_back.append(np.mean(fil_100_back_3[i]))\n",
    "    means_HToBB.append(np.mean(fil_100_HToBB_3[i]))\n",
    "    means_WToQQ.append(np.mean(fil_100_WToQQ_3[i]))\n",
    "    means_TTBar.append(np.mean(fil_100_TTBar_3[i]))\n",
    "\n",
    "    median_back.append(np.median(fil_100_back_3[i]))\n",
    "    median_HToBB.append(np.median(fil_100_HToBB_3[i]))\n",
    "    median_WToQQ.append(np.median(fil_100_WToQQ_3[i]))\n",
    "    median_TTBar.append(np.median(fil_100_TTBar_3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "30f5a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score BACKGROUND: 0.010935291821573379\n",
      "Estadísticas de las fidelidades medias para HToBB:\n",
      "Media: 97.9631863932231\n",
      "Mediana: 99.80948141693219\n",
      "Mínimo: 97.96314968539224\n",
      "Máximo: 97.9632176004691\n",
      "Desviación estándar: 1.3280851160762936e-05\n",
      "Anomaly Score: 0.020368503146077632\n",
      "\n",
      "Estadísticas de las fidelidades medias para WToQQ:\n",
      "Media: 98.81728877666805\n",
      "Mediana: 99.86475336791814\n",
      "Mínimo: 98.81723730903295\n",
      "Máximo: 98.81731600130546\n",
      "Desviación estándar: 1.4237139833855767e-05\n",
      "Anomaly Score: 0.011827626909670541\n",
      "\n",
      "Estadísticas de las fidelidades medias para TTBar:\n",
      "Media: 97.25351168519006\n",
      "Mediana: 99.60967275453382\n",
      "Mínimo: 97.25347624060994\n",
      "Máximo: 97.25353830731318\n",
      "Desviación estándar: 1.3109564762407157e-05\n",
      "Anomaly Score: 0.027465237593900627\n",
      "Difereferenicas entre medias\n",
      "HToBB - WToQQ: -0.8541023834449533\n",
      "HToBB - TTBar: 0.7096747080330346\n",
      "WToQQ - TTBar: 1.5637770914779878\n",
      "Difereferenicas entre medianas\n",
      "HToBB - WToQQ: -0.05527195098595428\n",
      "HToBB - TTBar: 0.19980866239836814\n",
      "WToQQ - TTBar: 0.2550806133843224\n"
     ]
    }
   ],
   "source": [
    "print(\"Anomaly Score BACKGROUND:\", 1 - np.min(means_back)/100)\n",
    "\n",
    "print(\"Estadísticas de las fidelidades medias para HToBB:\")\n",
    "print(\"Media:\", np.mean(means_HToBB))\n",
    "print(\"Mediana:\", np.median(median_HToBB))\n",
    "print(\"Mínimo:\", np.min(means_HToBB))\n",
    "print(\"Máximo:\", np.max(means_HToBB))\n",
    "print(\"Desviación estándar:\", np.std(means_HToBB))\n",
    "print(\"Anomaly Score:\", 1 - np.min(means_HToBB)/100)\n",
    "print()\n",
    "print(\"Estadísticas de las fidelidades medias para WToQQ:\")\n",
    "print(\"Media:\", np.mean(means_WToQQ))\n",
    "print(\"Mediana:\", np.median(median_WToQQ))\n",
    "print(\"Mínimo:\", np.min(means_WToQQ))\n",
    "print(\"Máximo:\", np.max(means_WToQQ))\n",
    "print(\"Desviación estándar:\", np.std(means_WToQQ))\n",
    "print(\"Anomaly Score:\", 1 - np.min(means_WToQQ)/100)\n",
    "print()\n",
    "print(\"Estadísticas de las fidelidades medias para TTBar:\")\n",
    "print(\"Media:\", np.mean(means_TTBar))\n",
    "print(\"Mediana:\", np.median(median_TTBar))\n",
    "print(\"Mínimo:\", np.min(means_TTBar))\n",
    "print(\"Máximo:\", np.max(means_TTBar))   \n",
    "print(\"Desviación estándar:\", np.std(means_TTBar))\n",
    "print(\"Anomaly Score:\", 1 - np.min(means_TTBar)/100)\n",
    "\n",
    "\n",
    "print(\"Difereferenicas entre medias\")\n",
    "print(\"HToBB - WToQQ:\", np.mean(means_HToBB) - np.mean(means_WToQQ))\n",
    "print(\"HToBB - TTBar:\", np.mean(means_HToBB) - np.mean(means_TTBar))\n",
    "print(\"WToQQ - TTBar:\", np.mean(means_WToQQ) - np.mean(means_TTBar))\n",
    "\n",
    "print(\"Difereferenicas entre medianas\")\n",
    "print(\"HToBB - WToQQ:\", np.median(median_HToBB) - np.median(median_WToQQ))\n",
    "print(\"HToBB - TTBar:\", np.median(median_HToBB) - np.median(median_TTBar))\n",
    "print(\"WToQQ - TTBar:\", np.median(median_WToQQ) - np.median(median_TTBar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./fidelidades_100.npz', allow_pickle=True)\n",
    "fil_100_back_4auc= data['fil_100_back_S']\n",
    "fil_100_HToBB_4auc = data['fil_100_HToBB_S']\n",
    "fil_100_WToQQ_4auc = data['fil_100_WToQQ_S']\n",
    "fil_100_TTBar_4auc = data['fil_100_TTBar_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf73fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC HToBB: 0.7003041905\n",
      "Mean AUC TTBar: 0.7890799033999998\n",
      "Mean AUC WToQQ: 0.6482334243000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "auc_HToBB_list = []\n",
    "auc_TTBar_list = []\n",
    "auc_WToQQ_list = []\n",
    "\n",
    "fil_100_back_4auc = fil_100_back_4auc.squeeze()\n",
    "fil_100_HToBB_4auc = fil_100_HToBB_4auc.squeeze()\n",
    "fil_100_WToQQ_4auc = fil_100_WToQQ_4auc.squeeze()\n",
    "fil_100_TTBar_4auc = fil_100_TTBar_4auc.squeeze()\n",
    "\n",
    "for i in range(100):\n",
    "    event_fidelities_back = fil_100_back_4auc[i]\n",
    "    event_fidelities_HToBB = fil_100_HToBB_4auc[i]\n",
    "    event_fidelities_TTBar = fil_100_TTBar_4auc[i]\n",
    "    event_fidelities_WToQQ = fil_100_WToQQ_4auc[i]\n",
    "\n",
    "    # --- Anomaly scores (1 - Fidelity) ---\n",
    "    anomaly_scores_back = 1 - np.array(event_fidelities_back).ravel()\n",
    "    anomaly_scores_HToBB = 1 - np.array(event_fidelities_HToBB).ravel()\n",
    "    anomaly_scores_TTBar = 1 - np.array(event_fidelities_TTBar).ravel()\n",
    "    anomaly_scores_WToQQ = 1 - np.array(event_fidelities_WToQQ).ravel() \n",
    "\n",
    "    # --- Calcular AUC ---\n",
    "    auc_HToBB = roc_auc_score(\n",
    "        np.concatenate([np.zeros_like(anomaly_scores_back), np.ones_like(anomaly_scores_HToBB)]),\n",
    "        np.concatenate([anomaly_scores_back, anomaly_scores_HToBB])\n",
    "    )\n",
    "\n",
    "    auc_TTBar = roc_auc_score(\n",
    "        np.concatenate([np.zeros_like(anomaly_scores_back), np.ones_like(anomaly_scores_TTBar)]),\n",
    "        np.concatenate([anomaly_scores_back, anomaly_scores_TTBar])\n",
    "    )\n",
    "\n",
    "    auc_WToQQ = roc_auc_score(\n",
    "        np.concatenate([np.zeros_like(anomaly_scores_back), np.ones_like(anomaly_scores_WToQQ)]),\n",
    "        np.concatenate([anomaly_scores_back, anomaly_scores_WToQQ])\n",
    "    )\n",
    "\n",
    "    auc_HToBB_list.append(auc_HToBB)\n",
    "    auc_TTBar_list.append(auc_TTBar)\n",
    "    auc_WToQQ_list.append(auc_WToQQ)\n",
    "\n",
    "#medias de AUCs\n",
    "mean_auc_HToBB = np.mean(auc_HToBB_list)\n",
    "mean_auc_TTBar = np.mean(auc_TTBar_list)\n",
    "mean_auc_WToQQ = np.mean(auc_WToQQ_list)\n",
    "\n",
    "\n",
    "print(\"Mean AUC HToBB:\", mean_auc_HToBB)\n",
    "print(\"Mean AUC TTBar:\", mean_auc_TTBar)\n",
    "print(\"Mean AUC WToQQ:\", mean_auc_WToQQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c255a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadísticas de las AUC medias para HToBB:\n",
      "Media: 0.7003041905\n",
      "Mínimo: 0.7002975\n",
      "Máximo: 0.7003258\n",
      "Desviación estándar: 3.6216687797182165e-06\n",
      "\n",
      "Estadísticas de las AUC  medias para WToQQ:\n",
      "Media: 0.6482334243000001\n",
      "Mínimo: 0.64821266\n",
      "Máximo: 0.64824222\n",
      "Desviación estándar: 3.3007757436707747e-06\n",
      "\n",
      "Estadísticas de las AUC  medias para TTBar:\n",
      "Media: 0.7890799033999998\n",
      "Mínimo: 0.78905882\n",
      "Máximo: 0.78908506\n",
      "Desviación estándar: 3.166291275285535e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Estadísticas de las AUC medias para HToBB:\")\n",
    "print(\"Media:\", np.mean(auc_HToBB_list))\n",
    "print(\"Mínimo:\", np.min(auc_HToBB_list))\n",
    "print(\"Máximo:\", np.max(auc_HToBB_list))\n",
    "print(\"Desviación estándar:\", np.std(auc_HToBB_list))\n",
    "print()\n",
    "print(\"Estadísticas de las AUC  medias para WToQQ:\")\n",
    "print(\"Media:\", np.mean(auc_WToQQ_list))\n",
    "print(\"Mínimo:\", np.min(auc_WToQQ_list))\n",
    "print(\"Máximo:\", np.max(auc_WToQQ_list))\n",
    "print(\"Desviación estándar:\", np.std(auc_WToQQ_list))\n",
    "print()\n",
    "print(\"Estadísticas de las AUC  medias para TTBar:\")\n",
    "print(\"Media:\", np.mean(auc_TTBar_list))\n",
    "print(\"Mínimo:\", np.min(auc_TTBar_list))\n",
    "print(\"Máximo:\", np.max(auc_TTBar_list))   \n",
    "print(\"Desviación estándar:\", np.std(auc_TTBar_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
